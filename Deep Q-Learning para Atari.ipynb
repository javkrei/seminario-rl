{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Lambda\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import sklearn.preprocessing\n",
    "from lib import plotting\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "ENV_NAME = \"Pong-v0\"\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "# fuentes:\n",
    "# https://github.com/rohitgirdhar/Deep-Q-Networks/\n",
    "# https://github.com/keon/deep-q-learning/blob/master/dqn.py\n",
    "# https://github.com/AdamStelmaszczyk/dqn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Red Neuronal que aproxima la función de valor estado acción (q-function)\n",
    "    \"\"\"   \n",
    "    def __init__(self, env, frame_history=4, learning_rate=0.001):        \n",
    "        n_actions = env.action_space.n\n",
    "        obs_shape = (84,84,frame_history)\n",
    "        model = Sequential()\n",
    "        model.add(Lambda(lambda x: x / 255.0))\n",
    "        \n",
    "        #se define una red convolucional\n",
    "        model.add(Conv2D(16, 8, strides=(4, 4),activation='relu',input_shape=obs_shape))\n",
    "        model.add(Conv2D(32, 4, strides=(2, 2),activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(n_actions, activation=None))\n",
    "        \n",
    "        # para la red convolucional como la utilizada en el paper de DeepMind descomentar\n",
    "        # (https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "        #model = Sequential()\n",
    "        #model.add(Lambda(lambda x: x / 255.0))\n",
    "        #model.add(Conv2D(32, 8, strides=(4, 4),activation='relu',input_shape=obs_shape))\n",
    "        #model.add(Conv2D(64, 4, strides=(2, 2),activation='relu'))\n",
    "        #model.add(Conv2D(64, 3, strides=(1, 1),activation='relu'))\n",
    "        #model.add(Flatten())\n",
    "        #model.add(Dense(512, activation='relu'))\n",
    "        #model.add(Dense(n_actions, activation=None))\n",
    "        \n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(learning_rate, rho=0.95, epsilon=0.01))\n",
    "        self.model = model\n",
    "     \n",
    "    def update(self, states, q_values, verbose=0):\n",
    "        \"\"\"\n",
    "        Realiza un update de los parámetros de la red neuronal usando un batch de estados y batch de vectores de valores\n",
    "        de la función estado-acción correspondientes\n",
    "        \"\"\"\n",
    "        self.model.fit(states, q_values, verbose=0)\n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Realiza una predicción de la función de valor estado-accion dado el estado\n",
    "        \n",
    "        Argumentos:\n",
    "            s: estado para el cual realizar la predicción\n",
    "            \n",
    "        Retorna:\n",
    "            Un vector con la predicción de la función de valor para todas las accións\n",
    "        \"\"\"\n",
    "        return self.model.predict(self.process_state_for_network(s))\n",
    "    \n",
    "    def process_state_for_network(self, state):\n",
    "        \"\"\"Scale, convert to greyscale and store as float32.\n",
    "        Basically same as process state for memory, but this time\n",
    "        outputs float32 images.\n",
    "        \"\"\"\n",
    "        state = state.astype('float')\n",
    "        return state\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, memory_size=100000, history_length=4):\n",
    "        self.history = np.zeros((84, 84, history_length))\n",
    "        self.history_length = history_length\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Guardar una tupla de estado, accion, recompensa, proximo estado, estado_terminal.\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def preprocess_image(self, state):\n",
    "        \"\"\"\n",
    "        Convertir la imagen a blanco y negro y reducirla a 84x84.\n",
    "        \"\"\"\n",
    "        I = Image.fromarray(state, 'RGB')\n",
    "        I = I.convert('L')  # to gray\n",
    "        I = I.resize((84, 84), Image.ANTIALIAS)\n",
    "        I = np.array(I).astype('uint8')\n",
    "        return I\n",
    "    \n",
    "    def push_frame(self, state):\n",
    "        \"\"\"\n",
    "        Pushear el frame a la stack de frames.\n",
    "        \"\"\"\n",
    "        state = self.preprocess_image(state)\n",
    "        self.history[..., 0] = state\n",
    "        self.history = np.roll(self.history, -1, axis=-1)\n",
    "        return self.history.copy()\n",
    "    \n",
    "    def reset_history(self):\n",
    "        \"\"\"\n",
    "        Resetear el stack de frames.\n",
    "        \"\"\"\n",
    "        self.history = np.zeros((84, 84, self.history_length))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, env, estimator, estimator_target, num_steps, discount_factor=1.0, \n",
    "                 exploration_max=1.0, exploration_min=0.05, epsilon_decay=0.99, memory_size=100000, \n",
    "                 batch_size=64, target_update_freq=1000, history_length=4, train_freq=4, burnin=100000,\n",
    "                 save_freq = 50000):\n",
    "        \"\"\"\n",
    "        Algoritmo q-learning/sarsa con experience replay utilizando aproximación de funciones.\n",
    "\n",
    "        Argumentos de inicialización:\n",
    "            env: ambiente de OpenAI.\n",
    "            estimator: función de aproximación de la función de valor estado-acción.\n",
    "            estimator_target: función de aproximación target de la función de valor estado-acción.\n",
    "            num_steps: número de pasos durante los cuales entrenar el estimador.\n",
    "            discount_factor: factor de descuento gama.\n",
    "            exploration_max: probabilidad de tomar una acción aleatoria inicialmente.\n",
    "            exploration_min: mínima probabilidad de tomar una acción aleatoria.\n",
    "            epsilon_decay: luego de cada episodio la probabilidad de una acción aleatoria decae por este factor.\n",
    "            target_update_freq: frecuencia de update de la función de aproximación target.\n",
    "            train_freq: frecuencia de entrenamiento de la función de aproximación de la función de valor.\n",
    "            history_length: cantidad de frames que se mandan a la red neuronal.\n",
    "            burnin: cantidad inicial de pasos exploratorios para tener la memoria inicializada.\n",
    "            save_freq: cada cuántos pasos se guardan los modelos.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.estimator_target = estimator_target\n",
    "        \n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.train_freq = train_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        self.memory = Memory(memory_size, history_length)\n",
    "        \n",
    "        self.history_length = history_length\n",
    "        \n",
    "        self.burnin = burnin\n",
    "        self.save_freq = save_freq\n",
    "        \n",
    "        \n",
    "    def decay_exploration_rate(self):\n",
    "        # decae la probabilidad de exploración\n",
    "        self.exploration_rate *= self.epsilon_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "    \n",
    "    def policy_fn(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Política epsilon-greedy basada en la función de aproximación actual y la probabilidad de exploración epsilon.\n",
    "\n",
    "        Retorna:\n",
    "            Un vector con la probabilidad de tomar cada acción.\n",
    "        \"\"\"\n",
    "        A = np.ones(env.action_space.n, dtype=float) * self.exploration_rate / env.action_space.n\n",
    "        q_values = self.estimator.predict(state)\n",
    "        best_action = np.argmax(q_values[0])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"\n",
    "        Realiza una corrida de entrenamiento de la función de aproximación dado un batch de experiencia acumulada.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        # samplear de la memoria\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        batch_q_values = np.zeros((self.batch_size, self.env.action_space.n))\n",
    "        batch_states = np.zeros((self.batch_size, 84, 84, self.history_length))\n",
    "        \n",
    "        for i_batch, (state, action, reward, next_state, terminal) in enumerate(batch):\n",
    "            \n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                ## el update de q-learning para este ejemplo\n",
    "                q_update = reward + self.discount_factor * np.amax(self.estimator_target.predict(next_state)[0])\n",
    "            \n",
    "            q_values = self.estimator.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            batch_q_values[i_batch] = q_values\n",
    "            batch_states[i_batch] = state\n",
    "            i_batch += 1\n",
    "        \n",
    "        # un paso en la dirección del gradiente\n",
    "        self.estimator.update(batch_states, batch_q_values, verbose=0)\n",
    "        \n",
    "    def train(self):    \n",
    "        \"\"\"\n",
    "        Realiza Q-Learning o Sarsa con aproximación de la función de valor estado-acción.\n",
    "        Retorna:\n",
    "            Un objeto de tipo EpisodeStats con dos arrays de numpy para la longitud y recompensa acumulada de cada\n",
    "            episodio respectivamente.\n",
    "        \"\"\"\n",
    "        stats = plotting.EpisodeStats(\n",
    "            episode_lengths=[],\n",
    "            episode_rewards=[])    \n",
    "        \n",
    "        # realizamos self.burnin pasos iniciales de exploración para inicializar la memoria\n",
    "        itr = 0\n",
    "        while itr <= self.burnin:\n",
    "            step = 0\n",
    "            self.memory.reset_history()\n",
    "            state = env.reset()\n",
    "            # guardar en el stack de frames\n",
    "            state = self.memory.push_frame(state)\n",
    "            # hacer un reshape del estado para poder pasarlo por la red neronal\n",
    "            state = np.reshape(state, [1, 84, 84, self.history_length])\n",
    "            # resetear el reward\n",
    "            cum_reward = 0.0\n",
    "            # elegir acción al azar\n",
    "            action = np.random.choice(self.env.action_space.n)\n",
    "            while True:\n",
    "                env.render()\n",
    "                # tomar la acción elegida\n",
    "                next_state, reward, terminal, info = env.step(action)\n",
    "                cum_reward += reward*self.discount_factor**step\n",
    "                # guardar en el stack de frames\n",
    "                next_state = self.memory.push_frame(next_state)\n",
    "                itr += 1; step += 1\n",
    "                # hacer un reshape del estado para poder pasarlo por la red neronal\n",
    "                next_state = np.reshape(next_state, [1, 84, 84, self.history_length])\n",
    "                # elegir la próxima acción al azar\n",
    "                if not terminal:\n",
    "                    next_action = np.random.choice(self.env.action_space.n)\n",
    "                else:\n",
    "                    next_action = None\n",
    "                    self.memory.reset_history()\n",
    "                # almacenar en la memoria\n",
    "                self.memory.remember(state, action, reward, next_state, terminal)\n",
    "                # actualizar estado y acción\n",
    "                state = next_state; action = next_action\n",
    "                if terminal:\n",
    "                    print(\"Iteración de burnin:\" + str(itr) + \n",
    "                          \", Exploración: \" + str(self.exploration_rate) + \n",
    "                          \", Recompensa Acumulada: \" + str(cum_reward))\n",
    "                    break\n",
    "        \n",
    "        # comienza el loop de entrenamiento\n",
    "        itr = 0\n",
    "        step = 0\n",
    "        n_episode = 0 \n",
    "        while itr <=  self.num_steps:\n",
    "            # resetear el stack de frames (nuevo episodio)\n",
    "            self.memory.reset_history()\n",
    "            state = env.reset()\n",
    "            # guardar en el stack de frames\n",
    "            state = self.memory.push_frame(state)\n",
    "            # hacer un reshape del estado para poder pasarlo por la red neronal\n",
    "            state = np.reshape(state, [1, 84, 84, self.history_length])\n",
    "            cum_reward = 0.0    \n",
    "            # elegir la acción\n",
    "            action = np.random.choice(self.env.action_space.n, p=self.policy_fn(state, self.exploration_rate))\n",
    "            while True:\n",
    "                env.render()\n",
    "                # realizar un paso con la acción elegida\n",
    "                next_state, reward, terminal, info = env.step(action)\n",
    "                # acumular la recompensa\n",
    "                cum_reward += reward*self.discount_factor**step\n",
    "                # guardar en el stack de frames\n",
    "                next_state = self.memory.push_frame(next_state)\n",
    "                itr += 1; step += 1\n",
    "                # hacer un reshape del estado para poder pasarlo por la red nueronal\n",
    "                next_state = np.reshape(next_state, [1, 84, 84, self.history_length])\n",
    "                if not terminal:\n",
    "                    # elegir la próxima acción\n",
    "                    next_action = np.random.choice(self.env.action_space.n, p=self.policy_fn(next_state, self.exploration_rate))\n",
    "                else:\n",
    "                    next_action = None\n",
    "                    self.memory.reset_history()\n",
    "                \n",
    "                self.memory.remember(state, action, reward, next_state, terminal)\n",
    "                # actualizar el estado y la acción\n",
    "                state = next_state; action = next_action\n",
    "                if terminal:\n",
    "                    print(\"Episodio: \" + str(n_episode) + \n",
    "                          \", Iteración:\" + str(itr) + \n",
    "                          \", Exploración: \" + str(self.exploration_rate) + \n",
    "                          \", Recompensa Acumulada: \" + str(cum_reward))\n",
    "                    # Actualizar estadísticas\n",
    "                    stats.episode_rewards.append(cum_reward)\n",
    "                    stats.episode_lengths.append(step)\n",
    "                    break        \n",
    "                    \n",
    "                # copiar los pesos de la red siendo entrenada al target\n",
    "                if itr % self.target_update_freq == 0:\n",
    "                    self.estimator_target.model.set_weights(self.estimator.model.get_weights())\n",
    "                # entrenar la red cada train_freq steps\n",
    "                if itr % self.train_freq == 0:\n",
    "                    self.experience_replay()\n",
    "                # guardar la red nueronal\n",
    "                if itr % self.save_freq == 0:\n",
    "                    self.estimator.model.save(\"model_\" + str(itr) + \".h5\")\n",
    "            # decaer la probabilidad de exploración\n",
    "            self.decay_exploration_rate()\n",
    "            n_episode += 1\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/javkrei/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "estimator = Estimator(env)\n",
    "estimator_target = Estimator(env)\n",
    "learner = Learner(env, estimator, estimator_target, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración de burnin:1126, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:2151, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:3457, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:4635, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:5660, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:6835, Exploración: 1.0, Recompensa Acumulada: -20.0\n",
      "Iteración de burnin:7936, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:9064, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:10252, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:11557, Exploración: 1.0, Recompensa Acumulada: -20.0\n",
      "Iteración de burnin:12888, Exploración: 1.0, Recompensa Acumulada: -20.0\n",
      "Iteración de burnin:14226, Exploración: 1.0, Recompensa Acumulada: -20.0\n",
      "Iteración de burnin:15565, Exploración: 1.0, Recompensa Acumulada: -19.0\n",
      "Iteración de burnin:16754, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:17955, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:19089, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:20214, Exploración: 1.0, Recompensa Acumulada: -20.0\n",
      "Iteración de burnin:21526, Exploración: 1.0, Recompensa Acumulada: -20.0\n",
      "Iteración de burnin:22629, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:23769, Exploración: 1.0, Recompensa Acumulada: -21.0\n",
      "Iteración de burnin:24979, Exploración: 1.0, Recompensa Acumulada: -21.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7055674170ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-9c630f22039a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# tomar la acción elegida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# draw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats = learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(env)\n",
    "estimator.model = keras.models.load_model(\"model_500000.h5\")\n",
    "estimator_target = Estimator(env)\n",
    "estimator_target.model = keras.models.load_model(\"model_500000.h5\")\n",
    "learner = Learner(env, estimator, estimator_target, 500000, discount_factor=1.0, exploration_max=0.2,  xploration_min=0.05,\n",
    "                  epsilon_decay=0.99, burnin=100000, save_freq = 100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
